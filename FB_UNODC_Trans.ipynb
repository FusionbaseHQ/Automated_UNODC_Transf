{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FusionBase_UNODC generic script:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adem\n",
      "succesfully created C:\\Users\\Adem/FBDirectory/\n",
      "succesfully created C:\\Users\\Adem/TransformedFilesDirectory/\n",
      "succesfully created C:\\Users\\Adem/FBFinalDirectory/\n"
     ]
    }
   ],
   "source": [
    "# Networking\n",
    "import requests\n",
    "\n",
    "from fb_country.utils import get_country_attribute\n",
    "\n",
    "\n",
    "\n",
    "# File handling\n",
    "import zipfile\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Data wrangling\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import hashlib\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Dateteime\n",
    "from datetime import datetime\n",
    "#update\n",
    "from apscheduler.schedulers.blocking import BlockingScheduler\n",
    "\n",
    "\n",
    "#GenericDirectories\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "home_directory = str(Path.home())\n",
    "print(home_directory)\n",
    "DownloadedFilesDirectory=home_directory+'/FBDirectory/'\n",
    "ExportedFilesDirectory=home_directory+'/TransformedFilesDirectory/'\n",
    "FinalDirectory=home_directory+'/FBFinalDirectory/'\n",
    "# define the name of the directory to be created\n",
    "Directories=[DownloadedFilesDirectory,ExportedFilesDirectory,FinalDirectory]\n",
    "for d in Directories:\n",
    "    if not(os.path.exists(d)):\n",
    "        try:\n",
    "            os.mkdir(d)\n",
    "            print(\"succesfully created \"+d)\n",
    "        except OSError:\n",
    "            print (\"Creation of the directory failed \" +d)\n",
    "    else:\n",
    "        print (\"Directory exists already \" +d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated Tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "succesfully created C:\\Users\\Adem/FBtempDirectory/\n",
      "succesfully created C:\\Users\\Adem/FBtempDirectory/\n",
      "succesfully created C:\\Users\\Adem/FBtempDirectory/\n"
     ]
    }
   ],
   "source": [
    "#Schedule the automated tasks to be done regularly for all possible data sources\n",
    "def update():\n",
    "    def _get_fb_id(row):\n",
    "        '''\n",
    "        Private util method to construct fb_id (fusionbase id) by hashing row values.\n",
    "        '''\n",
    "        concat_str = ''.join([str(x) for x in np.array(row)])\n",
    "\n",
    "        return hashlib.sha3_256(concat_str.encode('utf-8')).hexdigest()[:32]\n",
    "\n",
    "    TemporaryDirectory=home_directory+'/FBtempDirectory/'\n",
    "    try:\n",
    "        os.mkdir(TemporaryDirectory)\n",
    "        print(\"succesfully created \"+TemporaryDirectory)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory failed \"+TemporaryDirectory)\n",
    "        #Downloading data from sources\n",
    "    url=['car_theft.xlsx','theft_0.xlsx','bulglary.xlsx','money_laundering.xlsx','fraud.xlsx','cjs_court.xlsx','cjs_prosecution.xlsx','cjspolicepersonnel.xlsx','cjsp_convicted.xlsx','cjsp_prosecuted.xlsx','cjsp_fromal_contact.xlsx','personheld-mortality.xlsx','personsheld-unsentenced-length.xlsx','unsentenced.xlsx','total_unsentenced.xls','personsheld.xlsx','persons_held_total.xls','sexual_exploitation_0.xlsx','sexual_violence_0.xlsx','kidnapping_0.xlsx','robbery_0.xlsx','serious_assault_0.xlsx','armsseizedfoundsurrenderedbytracing.csv_.xlsx','wholesales-price.xlsx','drugrelateddeath.xlsx','prisionfacilities.xlsx','prison_capacity.xls','prisonstaff.xlsx','personsenteringprisons.xlsx','personheld-nationality.xlsx','intentional_homicide-bysexandage.xls','homicide-citizenship.xls','crime_related_situational_context.xls','ipfm.xls','homicidebyagesex.xlsx',\n",
    "         'peopleconvicted-tip.xlsx','detected-trafficking.xlsx','homicide_sex_citizenship.xls','city-homicide.xls','armsseizedbytype.xlsx','treatment.xlsx']   \n",
    "\n",
    "    try:\n",
    "        for u in url:\n",
    "            r = requests.get('https://dataunodc.un.org/sites/dataunodc.un.org/files/'+u, allow_redirects=True)  \n",
    "            open(DownloadedFilesDirectory+u, 'wb').write(r.content)\n",
    "    except Exception:\n",
    "        print('Could not download data') \n",
    "\n",
    "        #Extracting data from excel files\n",
    "    listcol_count=[]\n",
    "    for u in url:\n",
    "        df = pd.read_excel(DownloadedFilesDirectory+u, index_col=None, header=None)\n",
    "        #Calculating the number of columns and indexes and stock them in lists to distinguish columns and indexes \n",
    "        #by trying to measure the length of distinct values and compare it to the length of columns\n",
    "\n",
    "        col_count=0\n",
    "        ind_count=0\n",
    "        AllList=[[] for i in range(len(df))]\n",
    "        columnsList=[[] for i in range(len(df))]\n",
    "        indexList=[[] for i in range(len(df))]\n",
    "        for j in range(len(df.columns)):\n",
    "            for i in range(len(df)):\n",
    "\n",
    "                if((type(df.iloc[i,j])!=int and type(df.iloc[i,j])!=float and str(df.iloc[i,j])!='nan' \n",
    "                    and type(df.iloc[i,j])!=np.float64) or (i==0 and str(df.iloc[i,j])!='nan')):\n",
    "\n",
    "                        AllList[i].append(df.iloc[i,j])\n",
    "\n",
    "\n",
    "\n",
    "                if((type(df.iloc[i,j])!=int and type(df.iloc[i,j])!=float and str(df.iloc[i,j])!='nan')\n",
    "                    and type(df.iloc[i,j])!=np.float64):\n",
    "                        columnsList[j].append(df.iloc[i,j])\n",
    "                        if (len(columnsList[j])>len(df)/2 ):\n",
    "                            if (0<=i<len(df)-1):\n",
    "                                if (type(df.iloc[i+1,j])!=int and type(df.iloc[i+1,j])!=float):\n",
    "                                    col_count=j+1\n",
    "\n",
    "                            else: \n",
    "                                col_count=j+1\n",
    "\n",
    "                        indexList[i].append(df.iloc[i,j])\n",
    "                        if (len(indexList[i])>len(df.columns)/2):\n",
    "                            ind_count=i+1\n",
    "            #determine the indexes,the additional indexes, the columns and their names\n",
    "        if ind_count==0:\n",
    "            ind_count=1        \n",
    "        names=[]\n",
    "        additionalindex=[]\n",
    "        columns=[]\n",
    "        indexList=AllList[col_count:]\n",
    "        for i in range(ind_count):\n",
    "            if(i==0):\n",
    "\n",
    "                if(len(AllList[i])==len(df.columns)):\n",
    "                    additionalindex.append(list(Counter(AllList[0]).keys())[col_count:])\n",
    "                    names.append(AllList[i][:col_count])\n",
    "\n",
    "                else:\n",
    "                    additionalindex.append(list(Counter(AllList[0]).keys()))\n",
    "\n",
    "            elif(i==ind_count-1 and len(AllList[0])!=len(df)):\n",
    "                columns.append(AllList[i][col_count:])\n",
    "                names.append(AllList[i][:col_count])\n",
    "            elif(1<len(Counter(df.iloc[i]).keys()) <4 ):\n",
    "                additionalindex.append(AllList[i])\n",
    "\n",
    "        tuples=AllList[ind_count:]\n",
    "        tuplesfinal=[]\n",
    "        for i in range(len(tuples)):\n",
    "            for j in range(len(additionalindex)):\n",
    "                for k in range(len(additionalindex[j])):\n",
    "                    tuplesfinal.append(tuples[i]+[additionalindex[j][k]])\n",
    "\n",
    "        namesfinal=[]\n",
    "        for i in range(len(names)):\n",
    "            for j in range(len(additionalindex)):\n",
    "                namesfinal.append(names[i]+[j])\n",
    "        columnsFinal=[]\n",
    "        if columns:\n",
    "            columnsFinal=list(Counter(columns[0]).keys())\n",
    "            #determine the values to be diplayed\n",
    "        values=[]\n",
    "        if columnsFinal:\n",
    "            for i in range(ind_count,len(df)):\n",
    "                for j in range(col_count,len(df.columns)-1,len(columnsFinal)):\n",
    "                    row=[]\n",
    "                    for k in range(len(columnsFinal)):\n",
    "                        row.append(df.iloc[i,j+k])\n",
    "                    values.append(row)\n",
    "        else:\n",
    "            for i in range(ind_count,len(df)):\n",
    "                for j in range(col_count,len(df.columns),1):\n",
    "                    values.append(df.iloc[i,j])           \n",
    "                #transform the data to a pivot table\n",
    "        index = pd.MultiIndex.from_tuples(tuple(map(tuple,tuplesfinal)), names=namesfinal[0])\n",
    "\n",
    "        if columns:\n",
    "\n",
    "            df = pd.DataFrame(values, index=index, columns=list(Counter(columns[0]).keys()))\n",
    "        else:\n",
    "            df = pd.DataFrame(values, index=index)\n",
    "\n",
    "        listcol_count.append(col_count)\n",
    "        df.to_excel(TemporaryDirectory+u)\n",
    "        df=pd.read_excel(TemporaryDirectory+u)\n",
    "        date_time=[]\n",
    "                #Adding the datetime and country_iso_alpha3\n",
    "        for i in range(len(df)):\n",
    "            date_time.append(datetime.now())\n",
    "        df['fb_datetime']=date_time\n",
    "\n",
    "        alpha=[]\n",
    "        df['Country'] = df['Country'].astype(str)\n",
    "        for i in range(len(df['Country'])):\n",
    "            a=get_country_attribute(df['Country'][i],'alpha3')\n",
    "            alpha.append(a)\n",
    "        df.insert(col_count, 'country_iso_alpha3',alpha)\n",
    "\n",
    "        fb_id=[]\n",
    "        fb_id =  np.array([_get_fb_id(row) for row in df.values])\n",
    "        df.set_index(fb_id,inplace=True)\n",
    "        df.index.set_names('fb_id',inplace=True)\n",
    "        #store the transformed data in destination\n",
    "        df.columns = df.columns.astype(str)\n",
    "        df.to_parquet(ExportedFilesDirectory+u+'.parquet.gzip',compression='gzip',engine='fastparquet',index=True)\n",
    "    shutil.rmtree(TemporaryDirectory)\n",
    "#Transforming datasets which contains 2 levels of additional index\n",
    "    parquetIndlevel2=['car_theft.xlsx','theft_0.xlsx','bulglary.xlsx','money_laundering.xlsx','fraud.xlsx','cjs_court.xlsx','cjs_prosecution.xlsx','cjspolicepersonnel.xlsx','cjsp_convicted.xlsx','cjsp_prosecuted.xlsx','cjsp_fromal_contact.xlsx','personheld-mortality.xlsx','personsheld-unsentenced-length.xlsx','unsentenced.xlsx','total_unsentenced.xls','personsheld.xlsx','persons_held_total.xls','sexual_exploitation_0.xlsx','sexual_violence_0.xlsx','kidnapping_0.xlsx','robbery_0.xlsx','serious_assault_0.xlsx','armsseizedfoundsurrenderedbytracing.csv_.xlsx','wholesales-price.xlsx','drugrelateddeath.xlsx']\n",
    "    #Transforming datasets which contains  \n",
    "    for parquet, c in zip(parquetIndlevel2,listcol_count[:25]):\n",
    "\n",
    "        df=pd.read_parquet(ExportedFilesDirectory+parquet+'.parquet.gzip')\n",
    "        temp=df.iloc[0,0:c+1]\n",
    "        for i in range(len(df)):\n",
    "            for k in range(c+1):\n",
    "                if (df.iloc[i,k]!=None and df.iloc[i,k]!=\"nan\"):\n",
    "                    temp[k]=df.iloc[i,k]\n",
    "            for j in range(c+1):\n",
    "                df.iloc[i,j]=temp[j]\n",
    "        df.rename(columns={\"0\": \"year\"},inplace=True)\n",
    "        df.to_parquet(FinalDirectory+parquet+'.parquet.gzip',compression='gzip',engine='fastparquet',index=True)\n",
    "        #Transforming datasets which contains 1 levels of additional index\n",
    "    parquetIndlevel1=['prisionfacilities.xlsx','prison_capacity.xls','prisonstaff.xlsx','personsenteringprisons.xlsx','personheld-nationality.xlsx','intentional_homicide-bysexandage.xls','homicide-citizenship.xls','crime_related_situational_context.xls','ipfm.xls','homicidebyagesex.xlsx',\n",
    "         'peopleconvicted-tip.xlsx','detected-trafficking.xlsx','homicide_sex_citizenship.xls','city-homicide.xls','armsseizedbytype.xlsx','treatment.xlsx']\n",
    "    convicted=[]\n",
    "    trafficking=[]\n",
    "    number_city_homicide=[]\n",
    "    number_arm_seized=[]\n",
    "    number_treatment=[]\n",
    "    for parquet, c in zip(parquetIndlevel1,listcol_count[25:]):\n",
    "\n",
    "        if (parquet=='peopleconvicted-tip.xlsx'):\n",
    "            df=pd.read_parquet(ExportedFilesDirectory+'peopleconvicted-tip.xlsx.parquet.gzip')                       \n",
    "            for i in range(len(df)):\n",
    "                convicted.append('number of convicted people')\n",
    "            df.insert(loc=4, column='Indicator', value=convicted)\n",
    "            temp=df.iloc[0,0:c+1]\n",
    "            for i in range(len(df)):\n",
    "                for k in range(c+1):\n",
    "                    if (df.iloc[i,k]!=None and df.iloc[i,k]!=\"nan\"):\n",
    "                        temp[k]=df.iloc[i,k]\n",
    "                for j in range(c+1):\n",
    "                    df.iloc[i,j]=temp[j]\n",
    "            df.rename(columns={\"0\": \"year\",\"0.1\":\"Indicator value\" },inplace=True)\n",
    "            df.to_parquet(FinalDirectory+parquet+'.parquet.gzip',compression='gzip',engine='fastparquet',index=True)\n",
    "\n",
    "        elif (parquet=='detected-trafficking.xlsx'):\n",
    "            df=pd.read_parquet(ExportedFilesDirectory+'detected-trafficking.xlsx.parquet.gzip')                       \n",
    "            for i in range(len(df)):\n",
    "                trafficking.append('number of detected trafficking')\n",
    "            df.insert(loc=4, column='Indicator', value=trafficking)\n",
    "            temp=df.iloc[0,0:c+1]\n",
    "            for i in range(len(df)):\n",
    "                for k in range(c+1):\n",
    "                    if (df.iloc[i,k]!=None and df.iloc[i,k]!=\"nan\"):\n",
    "                        temp[k]=df.iloc[i,k]\n",
    "                for j in range(c+1):\n",
    "                    df.iloc[i,j]=temp[j]\n",
    "            df.rename(columns={\"0\": \"year\",\"0.1\":\"Indicator value\" },inplace=True)\n",
    "            df.to_parquet(FinalDirectory+parquet+'.parquet.gzip',compression='gzip',engine='fastparquet',index=True)\n",
    "\n",
    "        elif(parquet =='armsseizedbytype.xlsx'):\n",
    "            df=pd.read_parquet(ExportedFilesDirectory+'armsseizedbytype.xlsx.parquet.gzip')                       \n",
    "            for i in range(len(df)):\n",
    "                number_arm_seized.append('number of seized arms')\n",
    "            df.insert(loc=8, column='Indicator', value=number_arm_seized)\n",
    "            temp=df.iloc[0,0:c+1]\n",
    "            for i in range(len(df)):\n",
    "                for k in range(c+1):\n",
    "                    if (df.iloc[i,k]!=None and df.iloc[i,k]!=\"nan\"):\n",
    "                        temp[k]=df.iloc[i,k]\n",
    "                for j in range(c+1):\n",
    "                    df.iloc[i,j]=temp[j]\n",
    "            df.rename(columns={\"0\": \"year\",\"0.1\":\"Indicator value\" },inplace=True)              \n",
    "            df.to_parquet(FinalDirectory+parquet+'.parquet.gzip',compression='gzip',engine='fastparquet',index=True)\n",
    "\n",
    "        elif (parquet =='homicide_sex_citizenship.xls'):\n",
    "            df=pd.read_parquet(ExportedFilesDirectory+'homicide_sex_citizenship.xls.parquet.gzip')   \n",
    "            df.rename(columns={\"Variable\": \"Indicator\"},inplace=True)\n",
    "\n",
    "            temp=df.iloc[0,0:c+1]\n",
    "            for i in range(len(df)):\n",
    "                for k in range(c+1):\n",
    "                    if (df.iloc[i,k]!=None and df.iloc[i,k]!=\"nan\"):\n",
    "                        temp[k]=df.iloc[i,k]\n",
    "                for j in range(c+1):\n",
    "                    df.iloc[i,j]=temp[j]\n",
    "            df.rename(columns={\"0\": \"year\",\"0.1\":\"Indicator value\" },inplace=True)\n",
    "            df.to_parquet(FinalDirectory+parquet+'.parquet.gzip',compression='gzip',engine='fastparquet',index=True)\n",
    "\n",
    "        elif(parquet=='city-homicide.xls'):\n",
    "            df=pd.read_parquet(ExportedFilesDirectory+'city-homicide.xls.parquet.gzip')                       \n",
    "            for i in range(len(df)):\n",
    "                number_city_homicide.append('number of homicide')\n",
    "            df.insert(loc=5, column='Indicator', value=number_city_homicide)\n",
    "            temp=df.iloc[0,0:c+1]\n",
    "            for i in range(len(df)):\n",
    "                for k in range(c+1):\n",
    "                    if (df.iloc[i,k]!=None and df.iloc[i,k]!=\"nan\"):\n",
    "                        temp[k]=df.iloc[i,k]\n",
    "                for j in range(c+1):\n",
    "                    df.iloc[i,j]=temp[j]\n",
    "            df.rename(columns={\"0\": \"year\",\"0.1\":\"Indicator value\" },inplace=True)              \n",
    "            df.to_parquet(FinalDirectory+parquet+'.parquet.gzip',compression='gzip',engine='fastparquet',index=True)\n",
    "\n",
    "\n",
    "\n",
    "        elif(parquet=='treatment.xlsx'):\n",
    "            df=pd.read_parquet(ExportedFilesDirectory+'treatment.xlsx.parquet.gzip')                       \n",
    "            for i in range(len(df)):\n",
    "                number_treatment.append('number of treatment')\n",
    "            df.insert(loc=4, column='Indicator', value=number_treatment)\n",
    "            temp=df.iloc[0,0:c+1]\n",
    "            for i in range(len(df)):\n",
    "                for k in range(c+1):\n",
    "                    if (df.iloc[i,k]!=None and df.iloc[i,k]!=\"nan\"):\n",
    "                        temp[k]=df.iloc[i,k]\n",
    "                for j in range(c+1):\n",
    "                    df.iloc[i,j]=temp[j]\n",
    "            df.rename(columns={\"0\": \"year\",\"0.1\":\"Indicator value\" },inplace=True)              \n",
    "            df.to_parquet(FinalDirectory+parquet+'.parquet.gzip',compression='gzip',engine='fastparquet',index=True)\n",
    "        else:\n",
    "            df=pd.read_parquet(ExportedFilesDirectory+parquet+'.parquet.gzip')\n",
    "            temp=df.iloc[0,0:c+1]\n",
    "            for i in range(len(df)):\n",
    "                for k in range(c+1):\n",
    "                    if (df.iloc[i,k]!=None and df.iloc[i,k]!=\"nan\"):\n",
    "                        temp[k]=df.iloc[i,k]\n",
    "                for j in range(c+1):\n",
    "                    df.iloc[i,j]=temp[j]\n",
    "            df.rename(columns={\"0\": \"year\",\"0.1\":\"Indicator value\" },inplace=True)\n",
    "            df.to_parquet(FinalDirectory+parquet+'.parquet.gzip',compression='gzip',engine='fastparquet',index=True)\n",
    "            \n",
    "\n",
    "#scheduler = apscheduler.schedulers.blocking.BackgroundScheduler('apscheduler.job_defaults.max_instances':'2')\n",
    "scheduler = BlockingScheduler()\n",
    "scheduler.add_job(update, 'interval', minutes=30,max_instances=5)\n",
    "scheduler.start()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
